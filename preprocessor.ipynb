{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install (via pip3): gensim, nltk.  \n",
    "You will also need to run (with python3 in terminal)  \n",
    "`>>>import nltk`  \n",
    "`>>>nltk.download('stopwords')`  \n",
    "`>>>nltk.download('wordnet')`  \n",
    "`>>>nltk.download('omw-1.4')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      " 1    3685\n",
      "-1    2106\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open('stock_data.csv', encoding='utf8') as csvfile:\n",
    "    df = pd.read_csv(csvfile, delimiter=',')\n",
    "\n",
    "df.dropna(axis=0, how='any', inplace=True)                         # Excludes null-containing rows\n",
    "num_positive = df['Sentiment'].value_counts()[1]\n",
    "num_negative = df['Sentiment'].value_counts()[-1]\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "word_frequency_requirement = 0.0013*(df['Sentiment'].size) # the number of times a word has to appear to be given\n",
    "# it's own encoding. All words under this limit are encoded as the same 'unknown' word.\n",
    "sg = 0\n",
    "vector_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex removal of various undesirable parts of a tweet\n",
    "def clean_tweet(tweet):\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) # Twitter handle removal\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet) # URL removal\n",
    "  tweet = re.sub(r\"[']\", \"\", tweet) # Apostrophe removal\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet) # Remove symbols that are not alphabetic or sentence endings\n",
    "  tweet = re.sub(r\"([^a-zA-Z])\", r\" \\1 \", tweet) # Places spaces around sentence endings,\n",
    "  # so they are encoded as their own words, rather than being lumped in with other words.\n",
    "  tweet = re.sub(r\" +\", ' ', tweet) # Excess whitespace removal\n",
    "  tweet = tweet.lower() # Send tweet to lowercase\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word lemmatizer and stopwords list for sanitisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def sanitise(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = filter(lambda w: w not in stops, tweet.strip().split()) # Remove stopwords\n",
    "    return list(map(lemmatizer.lemmatize, tweet)) # Lemmatize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df['Text'].map(sanitise))\n",
    "model= Word2Vec(sentences, min_count=1, vector_size=vector_size, sg=sg)      # default size=100, sg=0 CBOW, min_count=5\n",
    "wv = model.wv # get word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "0       [[tensor(0.0115), tensor(0.0052), tensor(0.009...\n",
      "1       [[tensor(0.2940), tensor(0.1189), tensor(0.233...\n",
      "2       [[tensor(0.2940), tensor(0.1189), tensor(0.233...\n",
      "3       [[tensor(0.0067), tensor(0.0027), tensor(0.005...\n",
      "4       [[tensor(0.0643), tensor(0.0267), tensor(0.050...\n",
      "                              ...                        \n",
      "5786    [[tensor(0.1110), tensor(0.0445), tensor(0.087...\n",
      "5787    [[tensor(0.0804), tensor(0.0314), tensor(0.063...\n",
      "5788    [[tensor(0.0379), tensor(0.0162), tensor(0.029...\n",
      "5789    [[tensor(0.0010), tensor(0.0017), tensor(0.001...\n",
      "5790    [[tensor(0.2201), tensor(0.0922), tensor(0.182...\n",
      "Name: 0, Length: 5791, dtype: object\n"
     ]
    }
   ],
   "source": [
    "encoded_df = pd.DataFrame([\n",
    "    [[wv[word] for word in sentence] for sentence in sentences], # Encode each word of each tweet\n",
    "    df['Sentiment'].map(lambda x: torch.tensor([1]) if (x==1) else torch.tensor([0])) # Map positive and negative sentiment to class-indicative tensors\n",
    "]).T\n",
    "\n",
    "max_tweet_length = max(len(x) for x in encoded_df[0])\n",
    "print(max_tweet_length)\n",
    "# zero vector padding\n",
    "encoded_df[0] = encoded_df[0].map( lambda x: x + [[0]*vector_size] * (max_tweet_length - len(x)) )\n",
    "\n",
    "encoded_df[0] = encoded_df[0].map(lambda x: torch.FloatTensor(x))\n",
    "print(encoded_df[0])\n",
    "with open(\"encoded_dataframe\", \"wb\") as encoded_dataframe:\n",
    "    pickle.dump(encoded_df, encoded_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b941bf498f276488674bf31f1b0cc37176298e8d600eb280d450861b05bebb56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation guided by https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install (via pip3): torch, matplotlib, numpy, nltk.  \n",
    "You will also need to run (with python3 in terminal)  \n",
    "`>>>import nltk`  \n",
    "`>>>nltk.download('stopwords')`  \n",
    "`>>>nltk.download('wordnet')`  \n",
    "`>>>nltk.download('omw-1.4')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1    3685\n",
      "-1    2106\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader \n",
    "from functools import partial  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "with open('stock_data.csv', encoding='utf8') as csvfile:\n",
    "    df = pd.read_csv(csvfile, delimiter=',')\n",
    "\n",
    "df.dropna(axis=0, how='any', inplace=True)                         # Excludes null-containing rows\n",
    "print()\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "word_frequency_requirement = 8\n",
    "embed_dimension = 300 \n",
    "embed_max_norm = 1\n",
    "bow_size = 4\n",
    "cbow_batchsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex removal of various undesirable parts of a tweet\n",
    "def clean_tweet(tweet):\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) # Twitter handle removal\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet) # URL removal\n",
    "  tweet = re.sub(r\"[']\", \"\", tweet) # Apostrophe removal\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet) # Remove symbols that are not alphabetic or sentence endings\n",
    "  tweet = re.sub(r\"([^a-zA-Z])\", r\" \\1 \", tweet) # Places spaces around sentence endings,\n",
    "  # so they are encoded as their own words, rather than being lumped in with other words.\n",
    "  tweet = re.sub(r\" +\", ' ', tweet) # Excess whitespace removal\n",
    "  tweet = tweet.lower() # Send tweet to lowercase\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word lemmatizer and stopwords list for sanitisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def sanitise(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = filter(lambda w: w not in stops, tweet.strip().split()) # Remove stopwords\n",
    "    return list(map(lemmatizer.lemmatize, tweet)) # Lemmatize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kicker', 'watchlist', 'xide', 'tit', 'soq', 'pnk', 'cpw', 'bpz', 'aj', 'trade', 'method', 'method', 'see', 'prev', 'post']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kicker, watchlist, xide, tit, soq, pnk, cpw, ...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[user, aap, movie, ., return, fea, geed, indic...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[user, id, afraid, short, amzn, looking, like,...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[aap, user, current, downtrend, break, ., othe...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[monday, relative, weakness, ., nyx, win, tie,...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>[industry, body, cii, said, discoms, likely, s...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>[gold, price, slip, r, investor, book, profit,...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>[worker, bajaj, auto, agreed, wage, cut, perio...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[sharemarket, live, sensex, day, high, point, ...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>[sensex, nifty, climb, day, high, still, key, ...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4752 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     [kicker, watchlist, xide, tit, soq, pnk, cpw, ...   \n",
       "1     [user, aap, movie, ., return, fea, geed, indic...   \n",
       "2     [user, id, afraid, short, amzn, looking, like,...   \n",
       "3     [aap, user, current, downtrend, break, ., othe...   \n",
       "4     [monday, relative, weakness, ., nyx, win, tie,...   \n",
       "...                                                 ...   \n",
       "4747  [industry, body, cii, said, discoms, likely, s...   \n",
       "4748  [gold, price, slip, r, investor, book, profit,...   \n",
       "4749  [worker, bajaj, auto, agreed, wage, cut, perio...   \n",
       "4750  [sharemarket, live, sensex, day, high, point, ...   \n",
       "4751  [sensex, nifty, climb, day, high, still, key, ...   \n",
       "\n",
       "                   Sentiment  \n",
       "0     [tensor(1), tensor(0)]  \n",
       "1     [tensor(1), tensor(0)]  \n",
       "2     [tensor(1), tensor(0)]  \n",
       "3     [tensor(0), tensor(1)]  \n",
       "4     [tensor(0), tensor(1)]  \n",
       "...                      ...  \n",
       "4747  [tensor(0), tensor(1)]  \n",
       "4748  [tensor(0), tensor(1)]  \n",
       "4749  [tensor(1), tensor(0)]  \n",
       "4750  [tensor(1), tensor(0)]  \n",
       "4751  [tensor(1), tensor(0)]  \n",
       "\n",
       "[4752 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_df = pd.DataFrame([\n",
    "    df['Text'].map(sanitise),\n",
    "    df['Sentiment'].map(lambda x: torch.tensor([1,0]) if (x==1) else torch.tensor([0,1]))\n",
    "    ]).T\n",
    "    \n",
    "indexes = [i for i, x in enumerate(san_df['Text']) if len(x) <= 5]\n",
    "san_df.drop(indexes, inplace=True)\n",
    "san_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(san_df.Text[0])\n",
    "san_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6656),\n",
       " ('aap', 781),\n",
       " ('!', 687),\n",
       " ('user', 611),\n",
       " ('?', 430),\n",
       " ('short', 415),\n",
       " ('day', 375),\n",
       " ('stock', 363),\n",
       " ('today', 323),\n",
       " ('volume', 292)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter class counts number of appearances of all words\n",
    "word_count = Counter()\n",
    "for tweet in san_df['Text']:\n",
    "    word_count.update(tweet)\n",
    "        \n",
    "# Create a dictionary that maps words to their one-hot vector indices\n",
    "vocab = [word for word in word_count if word_count[word] >= word_frequency_requirement] # vocab contains all words meeting the word frequency requirement.\n",
    "\n",
    "dictionary = {word : i+1 for i, word in enumerate(vocab)} # dicionary is a mapping of each vocab word to its vector index.The +1 reserves the zero index.\n",
    "\n",
    "dictionary[None] = 0 # Index 0 is reserved to be a blanket classification for all words below the word frequency requirement.\n",
    "\n",
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length = max(len(x) for x in san_df['Text'])\n",
    "\n",
    "encoded_df = pd.DataFrame([[list(map(lambda w : dictionary.get(w, 0), tweet)) for tweet in san_df['Text']]]).T\n",
    "\n",
    "encoded_df[0] = encoded_df[0].map( lambda x: x + [0] * (max_tweet_length - len(x)) )\n",
    "\n",
    "onehot_df = pd.DataFrame([\n",
    "    [F.one_hot(torch.LongTensor(enc_tweet), len(dictionary)) for enc_tweet in encoded_df[0]],\n",
    "    san_df['Sentiment']\n",
    "    ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "\n",
    "        super(CBOW_Model, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dimension,\n",
    "            max_norm=embed_max_norm,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dimension,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = x.mean(axis=1)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cbow(batch, text_pipeline):\n",
    "     batch_input, batch_output = [], []\n",
    "     for text in batch:\n",
    "         text_tokens_ids = text_pipeline(text)\n",
    "         if len(text_tokens_ids) < bow_size * 2 + 1:\n",
    "             continue\n",
    "         if max_tweet_length:\n",
    "             text_tokens_ids = text_tokens_ids[:max_tweet_length]\n",
    "         for idx in range(len(text_tokens_ids) - bow_size * 2):\n",
    "             token_id_sequence = text_tokens_ids[idx : (idx + bow_size * 2 + 1)]\n",
    "             output = token_id_sequence.pop(bow_size)\n",
    "             input_ = token_id_sequence\n",
    "             batch_input.append(input_)\n",
    "             batch_output.append(output)\n",
    "     \n",
    "     batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "     batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "     return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m----> 2\u001b[0m          data_iter,\n\u001b[0;32m      3\u001b[0m          batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m      4\u001b[0m          shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,         \n\u001b[0;32m      5\u001b[0m          collate_fn\u001b[39m=\u001b[39mpartial(collate_cbow, text_pipeline\u001b[39m=\u001b[39mtext_pipeline),\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_iter' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "         onehot_df[0],\n",
    "         batch_size=batch_size,\n",
    "         shuffle=True,         \n",
    "         # collate_fn=partial(collate_cbow, text_pipeline=text_pipeline),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b941bf498f276488674bf31f1b0cc37176298e8d600eb280d450861b05bebb56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

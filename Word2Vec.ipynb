{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install (via pip3): torch, matplotlib, numpy, nltk.  \n",
    "You will also need to run (with python3 in terminal)  \n",
    "`>>>import nltk`  \n",
    "`>>>nltk.download('stopwords')`  \n",
    "`>>>nltk.download('wordnet')`  \n",
    "`>>>nltk.download('omw-1.4')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    3685\n",
      "-1    2106\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "with open('stock_data.csv', encoding='utf8') as csvfile:\n",
    "    df = pd.read_csv(csvfile, delimiter=',')\n",
    "\n",
    "df.dropna(axis=0, how='any', inplace=True)                         # Excludes null-containing rows\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "word_frequency_requirement = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "word_frequency_requirement = 8\n",
    "embed_dimension = 300 \n",
    "embed_max_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex removal of various undesirable parts of a tweet\n",
    "def clean_tweet(tweet):\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) # Twitter handle removal\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet) # URL removal\n",
    "  tweet = re.sub(r\"[']\", \"\", tweet) # Apostrophe removal\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet) # Remove symbols that are not alphabetic or sentence endings\n",
    "  tweet = re.sub(r\"([^a-zA-Z])\", r\" \\1 \", tweet) # Places spaces around sentence endings,\n",
    "  # so they are encoded as their own words, rather than being lumped in with other words.\n",
    "  tweet = re.sub(r\" +\", ' ', tweet) # Excess whitespace removal\n",
    "  tweet = tweet.lower() # Send tweet to lowercase\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word lemmatizer and stopwords list for sanitisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def sanitise(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = filter(lambda w: w not in stops, tweet.strip().split()) # Remove stopwords\n",
    "    return list(map(lemmatizer.lemmatize, tweet)) # Lemmatize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kicker', 'watchlist', 'xide', 'tit', 'soq', 'pnk', 'cpw', 'bpz', 'aj', 'trade', 'method', 'method', 'see', 'prev', 'post']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kicker, watchlist, xide, tit, soq, pnk, cpw, ...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[user, aap, movie, ., return, fea, geed, indic...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[user, id, afraid, short, amzn, looking, like,...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[aap, user, current, downtrend, break, ., othe...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[monday, relative, weakness, ., nyx, win, tie,...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>[industry, body, cii, said, discoms, likely, s...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>[gold, price, slip, r, investor, book, profit,...</td>\n",
       "      <td>[tensor(0), tensor(1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>[worker, bajaj, auto, agreed, wage, cut, perio...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[sharemarket, live, sensex, day, high, point, ...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>[sensex, nifty, climb, day, high, still, key, ...</td>\n",
       "      <td>[tensor(1), tensor(0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4752 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     [kicker, watchlist, xide, tit, soq, pnk, cpw, ...   \n",
       "1     [user, aap, movie, ., return, fea, geed, indic...   \n",
       "2     [user, id, afraid, short, amzn, looking, like,...   \n",
       "3     [aap, user, current, downtrend, break, ., othe...   \n",
       "4     [monday, relative, weakness, ., nyx, win, tie,...   \n",
       "...                                                 ...   \n",
       "4747  [industry, body, cii, said, discoms, likely, s...   \n",
       "4748  [gold, price, slip, r, investor, book, profit,...   \n",
       "4749  [worker, bajaj, auto, agreed, wage, cut, perio...   \n",
       "4750  [sharemarket, live, sensex, day, high, point, ...   \n",
       "4751  [sensex, nifty, climb, day, high, still, key, ...   \n",
       "\n",
       "                   Sentiment  \n",
       "0     [tensor(1), tensor(0)]  \n",
       "1     [tensor(1), tensor(0)]  \n",
       "2     [tensor(1), tensor(0)]  \n",
       "3     [tensor(0), tensor(1)]  \n",
       "4     [tensor(0), tensor(1)]  \n",
       "...                      ...  \n",
       "4747  [tensor(0), tensor(1)]  \n",
       "4748  [tensor(0), tensor(1)]  \n",
       "4749  [tensor(1), tensor(0)]  \n",
       "4750  [tensor(1), tensor(0)]  \n",
       "4751  [tensor(1), tensor(0)]  \n",
       "\n",
       "[4752 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_df = pd.DataFrame([\n",
    "    df['Text'].map(sanitise),\n",
    "    df['Sentiment'].map(lambda x: torch.tensor([1,0]) if (x==1) else torch.tensor([0,1]))\n",
    "    ]).T\n",
    "    \n",
    "indexes = [i for i, x in enumerate(san_df['Text']) if len(x) <= 5]\n",
    "san_df.drop(indexes, inplace=True)\n",
    "san_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(san_df.Text[0])\n",
    "san_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_frequency_requirement' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     word_count\u001b[39m.\u001b[39mupdate(tweet)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Create a dictionary that maps words to their one-hot vector indices\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m vocab \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_count \u001b[39mif\u001b[39;00m word_count[word] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m word_frequency_requirement] \u001b[39m# vocab contains all words meeting the word frequency requirement.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dictionary \u001b[39m=\u001b[39m {word : i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(vocab)} \u001b[39m# dicionary is a mapping of each vocab word to its vector index.The +1 reserves the zero index.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dictionary[\u001b[39mNone\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# Index 0 is reserved to be a blanket classification for all words below the word frequency requirement.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [5], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     word_count\u001b[39m.\u001b[39mupdate(tweet)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Create a dictionary that maps words to their one-hot vector indices\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m vocab \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_count \u001b[39mif\u001b[39;00m word_count[word] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m word_frequency_requirement] \u001b[39m# vocab contains all words meeting the word frequency requirement.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dictionary \u001b[39m=\u001b[39m {word : i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(vocab)} \u001b[39m# dicionary is a mapping of each vocab word to its vector index.The +1 reserves the zero index.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dictionary[\u001b[39mNone\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# Index 0 is reserved to be a blanket classification for all words below the word frequency requirement.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_frequency_requirement' is not defined"
     ]
    }
   ],
   "source": [
    "# Counter class counts number of appearances of all words\n",
    "word_count = Counter()\n",
    "for tweet in san_df['Text']:\n",
    "    word_count.update(tweet)\n",
    "        \n",
    "# Create a dictionary that maps words to their one-hot vector indices\n",
    "vocab = [word for word in word_count if word_count[word] >= word_frequency_requirement] # vocab contains all words meeting the word frequency requirement.\n",
    "\n",
    "dictionary = {word : i+1 for i, word in enumerate(vocab)} # dicionary is a mapping of each vocab word to its vector index.The +1 reserves the zero index.\n",
    "\n",
    "dictionary[None] = 0 # Index 0 is reserved to be a blanket classification for all words below the word frequency requirement.\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length = max(len(x) for x in san_df['Text'])\n",
    "\n",
    "encoded_df = pd.DataFrame([[list(map(lambda w : dictionary.get(w, 0), tweet)) for tweet in san_df['Text']]]).T\n",
    "\n",
    "encoded_df[0] = encoded_df[0].map( lambda x: x + [0] * (max_tweet_length - len(x)) )\n",
    "\n",
    "    # print(min([min(t) for t in encoded_df[0]]))\n",
    "    # print(min(dictionary.values())\n",
    "\n",
    "onehot_df = pd.DataFrame([\n",
    "    [F.one_hot(torch.LongTensor(enc_tweet), len(dictionary)+1) for enc_tweet in encoded_df[0]],\n",
    "    san_df['Sentiment']\n",
    "    ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "\n",
    "        super(CBOW_Model, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dimension,\n",
    "            max_norm=embed_max_norm,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dimension,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = x.mean(axis=1)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b941bf498f276488674bf31f1b0cc37176298e8d600eb280d450861b05bebb56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

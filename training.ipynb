{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmJwTg40gTHJ"
   },
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "# ! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eko8q7aOgTHM"
   },
   "source": [
    "# Fine-tune a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndK4oucTgTHN"
   },
   "source": [
    "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ðŸ¤— Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n",
    "\n",
    "* Fine-tune a pretrained model with ðŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer).\n",
    "* Fine-tune a pretrained model in TensorFlow with Keras.\n",
    "* Fine-tune a pretrained model in native PyTorch.\n",
    "\n",
    "<a id='data-processing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdvQzcW-gTHZ"
   },
   "source": [
    "## Train in native PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNTZM_2cgTHa"
   },
   "source": [
    "Create a `DataLoader` for your training and test datasets so you can iterate over batches of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install (via pip3): gensim, nltk.\n",
    "You will also need to run (with python3 in terminal)\n",
    ">>>import nltk\n",
    ">>>nltk.download('stopwords')\n",
    ">>>nltk.download('wordnet')\n",
    ">>>nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stock_data.csv', encoding='utf8') as csvfile:\n",
    "    df = pd.read_csv(csvfile, delimiter=',')\n",
    "\n",
    "df.dropna(axis=0, how='any', inplace=True)                         # Excludes null-containing rows\n",
    "num_positive = df['Sentiment'].value_counts()[1]\n",
    "\n",
    "df.loc[df['Sentiment'] == -1, 'Sentiment'] = 0\n",
    "num_negative = df['Sentiment'].value_counts()[0]\n",
    "print(df['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "word_frequency_requirement = 0.0013*(df['Sentiment'].size) # the number of times a word has to appear to be given\n",
    "# it's own encoding. All words under this limit are encoded as the same 'unknown' word.\n",
    "sg = 0\n",
    "# vector_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex removal of various undesirable parts of a tweet\n",
    "def clean_tweet(tweet):\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) # Twitter handle removal\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet) # URL removal\n",
    "  tweet = re.sub(r\"[']\", \"\", tweet) # Apostrophe removal\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet) # Remove symbols that are not alphabetic or sentence endings\n",
    "  tweet = re.sub(r\"([^a-zA-Z])\", r\" \\1 \", tweet) # Places spaces around sentence endings,\n",
    "  # so they are encoded as their own words, rather than being lumped in with other words.\n",
    "  tweet = re.sub(r\" +\", ' ', tweet) # Excess whitespace removal\n",
    "  tweet = tweet.lower() # Send tweet to lowercase\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word lemmatizer and stopwords list for sanitisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def sanitise(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = filter(lambda w: w not in stops, tweet.strip().split()) # Remove stopwords\n",
    "    return \" \".join(list(map(lemmatizer.lemmatize, tweet))) # Lemmatize words.\n",
    "\n",
    "# Hyperparameters\n",
    "train_proportion = 0.80\n",
    "batch_size = 8\n",
    "\n",
    "print(type(df))\n",
    "df['Text'] = df['Text'].map(sanitise)\n",
    "max_tweet_length = max(len(x) for x in df['Text'])\n",
    "print(max_tweet_length)\n",
    "train_df = df.sample(frac = 0.8)\n",
    "eval_df = df.drop(train_df.index)\n",
    "\n",
    "### convert to Huggingface dataset\n",
    "train_dataset = Dataset(pa.Table.from_pandas(train_df))\n",
    "eval_dataset = Dataset(pa.Table.from_pandas(eval_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfViZpdLgTHa"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_datasets = tokenized_train_datasets.remove_columns([\"Text\", \"__index_level_0__\"])\n",
    "tokenized_train_datasets = tokenized_train_datasets.rename_column(\"Sentiment\", \"labels\")\n",
    "tokenized_train_datasets.set_format(\"torch\")\n",
    "\n",
    "tokenized_eval_datasets = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_datasets = tokenized_eval_datasets.remove_columns([\"Text\", \"__index_level_0__\"])\n",
    "tokenized_eval_datasets = tokenized_eval_datasets.rename_column(\"Sentiment\", \"labels\")\n",
    "tokenized_eval_datasets.set_format(\"torch\")\n",
    "print(tokenized_eval_datasets)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = tokenized_train_datasets, batch_size = batch_size, shuffle = True)\n",
    "eval_dataloader = DataLoader(dataset = tokenized_eval_datasets, batch_size = batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRN8n_l4gTHb"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrPews4igTHc"
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRVV77LfgTHb"
   },
   "source": [
    "### Optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTdqH8YOgTHb"
   },
   "source": [
    "Create an optimizer and learning rate scheduler to fine-tune the model. Let's use the [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUyZT4u-gTHb"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6gcdF95gTHb"
   },
   "source": [
    "Create the default learning rate scheduler from [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ethtD3ygTHb"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3mNTokkgTHc"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "Get free access to a cloud GPU if you don't have one with a hosted notebook like [Colaboratory](https://colab.research.google.com/) or [SageMaker StudioLab](https://studiolab.sagemaker.aws/).\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Great, now you are ready to train! ðŸ¥³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgUOzclbgTHc"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVIl38CigTHc"
   },
   "outputs": [],
   "source": [
    "plot_loss = []\n",
    "plot_correct = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    batch_progress_bar = tqdm(total=len(train_dataloader))\n",
    "    epoch_loss = 0.\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.data.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        batch_progress_bar.update(1)\n",
    "        batch_progress_bar.set_description(\"Progress through epoch\")\n",
    "\n",
    "    batch_progress_bar.clear()\n",
    "\n",
    "    eval_batch_progress_bar = tqdm(total=len(eval_dataloader))\n",
    "    correct = 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct += torch.sum(predictions==batch[\"labels\"]).data.item()\n",
    "        eval_batch_progress_bar.update(1)\n",
    "        eval_batch_progress_bar.set_description(\"Epoch Accuracy evaluation progress\")\n",
    "\n",
    "    eval_batch_progress_bar.clear()\n",
    "\n",
    "    accuracy = correct/(num_negative+num_positive)*100\n",
    "    print(\"Epoch: %02d, Loss: %f, Accuracy: %.2f%%\" % (epoch+1, epoch_loss, accuracy) )\n",
    "\n",
    "    plot_loss.append(loss/len(train_dataloader))\n",
    "    plot_correct.append(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.plot(plot_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg. Loss per Epoch (on Training Set)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plot_correct)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy per Epoch (on Test Set)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8i_p9w_gTHd"
   },
   "source": [
    "<a id='additional-resources'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4AYQEQ9gTHd"
   },
   "source": [
    "## Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "208tHDZMgTHd"
   },
   "source": [
    "For more fine-tuning examples, refer to:\n",
    "\n",
    "- [ðŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) includes scripts\n",
    "  to train common NLP tasks in PyTorch and TensorFlow.\n",
    "\n",
    "- [ðŸ¤— Transformers Notebooks](https://huggingface.co/docs/transformers/main/en/notebooks) contains various notebooks on how to fine-tune a model for specific tasks in PyTorch and TensorFlow."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b941bf498f276488674bf31f1b0cc37176298e8d600eb280d450861b05bebb56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

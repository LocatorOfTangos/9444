{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import pickle\n",
    "\n",
    "# if torch.cuda.is_available(): dev = \"cuda\"\n",
    "# else: dev = \"cpu\"\n",
    "dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "train_proportion = 0.80\n",
    "hidden_layer_size = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - LSTM\n",
    "embedding_dim = 400\n",
    "hidden_dim = 50\n",
    "n_layers = 2\n",
    "vocab_size = 10215 # need to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_dataframe\", \"rb\") as encoded_dataframe:\n",
    "    encoded_df = pickle.load(encoded_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data and split into training and testing data\n",
    "train_dataset = encoded_df.sample(frac = train_proportion)\n",
    "test_dataset = encoded_df.drop(train_dataset.index)\n",
    "\n",
    "train_size = train_dataset.shape[0]\n",
    "test_size = test_dataset.shape[0]\n",
    "\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.stack(tuple(train_dataset[0])).type(torch.float32).to(device), torch.stack(tuple(train_dataset[1])).to(device))\n",
    "test_tensor = torch.utils.data.TensorDataset(torch.stack(tuple(test_dataset[0])).type(torch.float32).to(device), torch.stack(tuple(test_dataset[1])).to(device))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRN_model(nn.Module):\n",
    "    def __init__(self, num_input, num_hid, num_out):\n",
    "        super().__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.batch_size = 1\n",
    "        self.H0= nn.Parameter(torch.Tensor(num_hid))\n",
    "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid))\n",
    "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid))\n",
    "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid))\n",
    "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
    "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
    "\n",
    "        # Various initialisation schemes. Initialisation is important.\n",
    "        nn.init.zeros_(self.H0)\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        nn.init.xavier_normal_(self.U)\n",
    "        nn.init.zeros_(self.hid_bias)\n",
    "        nn.init.xavier_normal_(self.V)\n",
    "        nn.init.zeros_(self.out_bias)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        H0 = torch.tanh(self.H0)\n",
    "        return(H0.unsqueeze(0))\n",
    " \n",
    "    def forward(self, seq):\n",
    "        seq_size, _ = seq.size()\n",
    "        h_t = self.init_hidden()\n",
    "        for t in range(seq_size):\n",
    "            x_t = seq[t]\n",
    "            c_t = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
    "            h_t = torch.tanh(c_t)\n",
    "        output = h_t @ self.V + self.out_bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers, vocab_size, output_size, drop_prob=0.5):\n",
    "     \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.output_size = output_size\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.full_connected = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "                 )\n",
    "        return hidden\n",
    "        \n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        hidden = self.init_hidden()\n",
    "        embeds = self.embedding(sentence)\n",
    "        batch_size = sentence.size(0)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        output = self.dropout(lstm_out)\n",
    "        output = self.full_connected(output)\n",
    "        output = self.sig(output)\n",
    "        output = output.view(batch_size, -1)\n",
    "        output = output[:, -1]\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = SRN_model(len(encoded_df[0][0][0]),hidden_layer_size,2)\n",
    "net = LSTM_Classifier(embedding_dim, hidden_dim, n_layers, vocab_size, 2)\n",
    "\n",
    "num_positive = 3685\n",
    "num_negative = 2106\n",
    "\n",
    "net.to(device)\n",
    "weight = torch.FloatTensor([num_positive/num_negative, num_positive/num_positive]).to(device)\n",
    "\n",
    "# Negative log likelihood loss. Suited for classification tasks.\n",
    "criterion = F.nll_loss\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_Classifier(\n",
       "  (embedding): Embedding(10215, 400)\n",
       "  (lstm): LSTM(400, 50, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (full_connected): Linear(in_features=50, out_features=2, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, data, label):\n",
    "\n",
    "    loss = 0\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        # Forward\n",
    "        output, hidden = net(data[i])\n",
    "\n",
    "        # Apply output nonlinearity. Log_softmax chosen as it is suited for classification tasks\n",
    "        outputs.append(F.log_softmax(output, dim=1))\n",
    "    \n",
    "    loss = criterion(torch.cat(outputs, dim=0), torch.squeeze(label,1), weight=weight)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m): \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate proportion of the test set correctly predicted.\u001b[39;00m\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, criterion, optimizer, data, label)\u001b[0m\n\u001b[0;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Apply output nonlinearity. Log_softmax chosen as it is suited for classification tasks\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(F\u001b[38;5;241m.\u001b[39mlog_softmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36mLSTM_Classifier.forward\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[0;32m     26\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden()\n\u001b[1;32m---> 27\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m     lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embeds, hidden)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "plot_loss = []\n",
    "plot_correct = []\n",
    "\n",
    "num_batches = train_size//batch_size\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss = 0.\n",
    "\n",
    "    # Trains on every training data item individually each epoch\n",
    "    for data, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss += train(net, criterion, optimizer, data, label)\n",
    "\n",
    "    if (loss <= 0.001): break\n",
    "\n",
    "    # Evaluate proportion of the test set correctly predicted.\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        output = net(data[0])\n",
    "        if (torch.argmax(output.data) == label[0][0]): correct += 1\n",
    "    accuracy = correct/test_size*100\n",
    "\n",
    "    # Append loss and accuracy results to lists for later plotting.\n",
    "    plot_loss.append(loss/num_batches)\n",
    "    plot_correct.append(accuracy)\n",
    "    \n",
    "    # Print loss and accuracy every epoch.\n",
    "    print(\"Epoch %02d, loss = %f, accuracy = %.2f%%\" % (e+1, loss / num_batches, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.plot(plot_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg. Loss per Epoch (on Training Set)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plot_correct)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy per Epoch (on Test Set)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b941bf498f276488674bf31f1b0cc37176298e8d600eb280d450861b05bebb56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

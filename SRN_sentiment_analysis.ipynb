{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sourced from https://www.kaggle.com/datasets/utkarshxy/stock-markettweets-lexicon-data  \n",
    "Some data processing abridged from https://www.kaggle.com/code/juniorbueno/stock-market-sentimen-bert-tokenizer  \n",
    "Various code snippets from COMP9444 assignment 'paraphrased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install (via pip3): torch, matplotlib, numpy, nltk.  \n",
    "You will also need to run (with python3 in terminal)  \n",
    "`>>>import nltk`  \n",
    "`>>>nltk.download('stopwords')`  \n",
    "`>>>nltk.download('wordnet')`  \n",
    "`>>>nltk.download('omw-1.4')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "\n",
    "with open('stock_data.csv', encoding='utf8') as csvfile:\n",
    "    df = pd.read_csv(csvfile, delimiter=',')\n",
    "\n",
    "df.dropna(axis=0, how='any', inplace=True)                         # Excludes null-containing rows\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "word_frequency_requirement = 4 # the number of times a word has to appear to be given\n",
    "# it's own encoding. All words under this limit are encoded as the same 'unknown' word.\n",
    "train_proportion = 0.8\n",
    "hidden_layer_size = 70\n",
    "learning_rate = 0.001\n",
    "#batch_size = 32 # Batch size 1 only for now.\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex removal of various undesirable parts of a tweet\n",
    "def clean_tweet(tweet):\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) # Twitter handle removal\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet) # URL removal\n",
    "  tweet = re.sub(r\"[']\", \"\", tweet) # Apostrophe removal\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet) # Remove symbols that are not alphabetic or sentence endings\n",
    "  tweet = re.sub(r\"([^a-zA-Z])\", r\" \\1 \", tweet) # Places spaces around sentence endings,\n",
    "  # so they are encoded as their own words, rather than being lumped in with other words.\n",
    "  tweet = re.sub(r\" +\", ' ', tweet) # Excess whitespace removal\n",
    "  tweet = tweet.lower() # Send tweet to lowercase\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word lemmatizer and stopwords list for sanitisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = filter(lambda w: w not in stops, tweet.strip().split()) # Remove stopwords\n",
    "    return list(map(lemmatizer.lemmatize, tweet)) # Lemmatize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_df = pd.DataFrame([\n",
    "    df['Text'].map(tokenize),\n",
    "    df['Sentiment'].map(lambda x: torch.tensor([1,0]) if (x==1) else torch.tensor([0,1]))\n",
    "    ]).T\n",
    "    \n",
    "indexes = [i for i, x in enumerate(san_df['Text']) if len(x) <= 5]\n",
    "san_df.drop(indexes, inplace=True)\n",
    "san_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(san_df.Text[0])\n",
    "san_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter class counts number of appearances of all words\n",
    "word_count = Counter()\n",
    "for tweet in san_df['Text']:\n",
    "    word_count.update(tweet)\n",
    "        \n",
    "# Create a dictionary that maps words to their one-hot vector indices\n",
    "vocab = [word for word in word_count if word_count[word] >= word_frequency_requirement] # vocab contains all words meeting the word frequency requirement.\n",
    "\n",
    "dictionary = {word : i+1 for i, word in enumerate(vocab)} # dicionary is a mapping of each vocab word to its vector index.The +1 reserves the zero index.\n",
    "\n",
    "dictionary[None] = 0 # Index 0 is reserved to be a blanket classification for all words below the word frequency requirement.\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length = max(len(x) for x in san_df['Text'])\n",
    "\n",
    "encoded_df = pd.DataFrame([[list(map(lambda w : dictionary.get(w, 0), tweet)) for tweet in san_df['Text']]]).T\n",
    "\n",
    "encoded_df[0] = encoded_df[0].map( lambda x: x + [0] * (max_tweet_length - len(x)) )\n",
    "\n",
    "    # print(min([min(t) for t in encoded_df[0]]))\n",
    "    # print(min(dictionary.values())\n",
    "\n",
    "onehot_df = pd.DataFrame([\n",
    "    [F.one_hot(torch.LongTensor(enc_tweet), len(dictionary)+1) for enc_tweet in encoded_df[0]],\n",
    "    san_df['Sentiment']\n",
    "    ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data and split into training and testing data\n",
    "full_dataset = list(zip(one_hot_encoded_tweets, sentiment))\n",
    "\n",
    "shuffle(full_dataset)\n",
    "\n",
    "train_size = int(train_proportion * len(full_dataset))\n",
    "\n",
    "train_dataset = full_dataset[:train_size]\n",
    "test_dataset = full_dataset[train_size:]\n",
    "\n",
    "train_dataset = list(zip(*train_dataset))\n",
    "test_dataset = list(zip(*test_dataset))\n",
    "\n",
    "# Training data and Training labels are kept as nested lists rather than tensors where possible,\n",
    "# as tweets have varying length. This prevents the full data set from being represented as a pytorch\n",
    "# tensor, which requires that all dimensions of the tensor must be equal.\n",
    "# Yes, this sucks. It can be avoided in the future by padding sequences.\n",
    "tr_data = train_dataset[0]\n",
    "tr_label = train_dataset[1]\n",
    "\n",
    "te_data_tensor = list(map(torch.FloatTensor, test_dataset[0]))\n",
    "te_label_tensor = torch.FloatTensor(test_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRN_model(nn.Module):\n",
    "    def __init__(self, num_input, num_hid, num_out):\n",
    "        super().__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.batch_size = 1\n",
    "        self.H0= nn.Parameter(torch.Tensor(num_hid))\n",
    "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid))\n",
    "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid))\n",
    "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid))\n",
    "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
    "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
    "\n",
    "        # Various initialisation schemes. Initialisation is important.\n",
    "        nn.init.zeros_(self.H0)\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        nn.init.xavier_normal_(self.U)\n",
    "        nn.init.zeros_(self.hid_bias)\n",
    "        nn.init.xavier_normal_(self.V)\n",
    "        nn.init.zeros_(self.out_bias)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        H0 = torch.tanh(self.H0)\n",
    "        return(H0.unsqueeze(0))\n",
    " \n",
    "    def forward(self, seq):\n",
    "        seq_size, _ = seq.size()\n",
    "        h_t = self.init_hidden().to(seq.device)\n",
    "        for t in range(seq_size):\n",
    "            x_t = seq[t]\n",
    "            c_t = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
    "            h_t = torch.tanh(c_t)\n",
    "        output = h_t @ self.V + self.out_bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, data, label):\n",
    "    net.init_hidden()\n",
    "\n",
    "    # Forward\n",
    "    output = net(data)\n",
    "\n",
    "    # Apply output nonlinearity. Log_softmax chosen as it is suited for classification tasks\n",
    "    log_prob = F.log_softmax(output, dim=1)\n",
    "    \n",
    "    loss = criterion(log_prob[0], label)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of correct predictions the model can perform on the testing set\n",
    "def predict(net, test_data, test_label):\n",
    "    correct = 0\n",
    "    for i in range(len(test_data)):\n",
    "        output = net(test_data[i])\n",
    "        if (test_label[i][torch.argmax(output.data)] == 1): correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SRN_model(len(dictionary),hidden_layer_size,3)\n",
    "\n",
    "# Negative log likelihood loss. Suited for classification tasks.\n",
    "criterion = F.nll_loss\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss = []\n",
    "plot_correct = []\n",
    "\n",
    "num_examples = len(tr_data)\n",
    "num_batches = num_examples\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss = 0.\n",
    "\n",
    "    # Shuffle the dataset and convert training data sequences to FloatTensors right before training.\n",
    "    # Converting them to FloatTensors earlier causes bugs in the zip function. Frustratingly.\n",
    "    full_training_dataset = list(zip(tr_data, tr_label))\n",
    "    shuffle(full_training_dataset)\n",
    "    shuffled_training_dataset = list(zip(*full_training_dataset))\n",
    "\n",
    "    tr_data_tensor  = list(map(torch.FloatTensor, shuffled_training_dataset[0]))\n",
    "    \n",
    "    tr_label_tensor = torch.LongTensor(shuffled_training_dataset[1])\n",
    "\n",
    "    # Trains on every training data item individually each epoch\n",
    "    for i in range(num_examples):\n",
    "        loss += train(net, criterion, optimizer, tr_data_tensor[i], tr_label_tensor[i])\n",
    "\n",
    "    # Evaluate proportion of the test set correctly predicted.\n",
    "    correct = predict(net, te_data_tensor, te_label_tensor)/len(te_data_tensor)*100\n",
    "\n",
    "    # Append loss and accuracy results to lists for later plotting.\n",
    "    plot_loss.append(loss/num_batches)\n",
    "    plot_correct.append(correct)\n",
    "    \n",
    "    # Print loss and accuracy every epoch.\n",
    "    print(\"Epoch %02d, loss = %f, accuracy = %.2f%%\" % (e+1, loss / num_batches, correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.plot(plot_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg. Loss per Epoch (on Training Set)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plot_correct)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy per Epoch (on Test Set)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b941bf498f276488674bf31f1b0cc37176298e8d600eb280d450861b05bebb56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
